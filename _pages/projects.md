---
layout: archive
title: "Projects"
permalink: /projects/
author_profile: true
---

<!-- {% if author.googlescholar %}
  You can also find my articles on <u><a href="{{author.googlescholar}}">my Google Scholar profile</a>.</u>
{% endif %}

{% include base_path %}

{% for post in site.publications reversed %}
  {% include archive-single.html %}
{% endfor %} -->

## EasyDistill Open-source Toolkit
* Paper: [[pdf](https://arxiv.org/pdf/2505.20888)].
* Project Link: [https://github.com/modelscope/easydistill](https://github.com/modelscope/easydistill).
* Time: From 2025-05-29 to Now.

> EasyDistill is an easy-to-use NLP development and application toolkit in PyTorch, first released inside Alibaba in 2025. It is built with scalable distributed training strategies and supports both black-box and white-box methodologies. It facilitates efficient model training, enabling smaller models to emulate the performance of larger ones without compromising accuracy. EasyDistill boasts an extensive range of features, including data synthesis, supervised fine-tuning, ranking optimization, and reinforcement learning, all tailored for various KD scenarios.

## EasyNLP Open-source Toolkit
* Paper: [[pdf](https://aclanthology.org/2022.emnlp-demos.3.pdf)].
* Project Link: [https://github.com/alibaba/EasyNLP](https://github.com/alibaba/EasyNLP).
* Time: From 2022-04-10 to Now.

> EasyNLP integrates knowledge distillation and few-shot learning for landing large pre-trained models, together with various popular multi-modality pre-trained models. It provides a unified framework of model training, inference, and deployment for real-world applications. 

>It has powered more than 10 BUs and more than 20 business scenarios within the Alibaba group. It is seamlessly integrated to Platform of AI (PAI) products, including PAI-DSW for development, PAI-DLC for cloud-native training, PAI-EAS for serving, and PAI-Designer for zero-code model training.
